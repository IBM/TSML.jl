```@meta
Author = "Paulito P. Palmes"
```

# TS Data Discovery

We have enough building blocks to perform data discovery given a bunch 
of time series data generated by sensors. Processing hundreds or thousands
of time series data is becoming a common occurrence and typical challenge nowadays
with the rapid adoption of IoT technology in buildings, manufacturing industries, etc.

In this section, we will use those transformers discussed in the previous sections to normalize
and extract the statistical features of TS. These extracted stat features will be used
as input to a Machine learning model. We will train this model to learn the signatures of different
TS types so that we can use it to classify unknown or unlabeled sensor data.

In this tutorial, we will use `TSClassifier` which works in the following context: 
Given a bunch of time-series with specific types. Get the statistical features of each,
use these as inputs to a classifier with output as the TS type, train, and test. Another
option is to use these stat features for clustering and check cluster quality. If
accuracy is poor, add more stat features and repeat same process as outlined for training
and testing. Assume that each time series during training is named based on their type which will be
used as the target output. For example, temperature time series will be named as temperature?.csv
where ? is any positive integer. Using this setup, the `TSClassifier` loops over each file in the
`training` directory, get the stats and record these accumulated stat features into a dataframe
and train the model to learn the input->output mapping during `fit!` operation. Apply the learned
models in the `transform!` operation loading files in the `testing` directory.

The entire process of training to learn the appropriate parameters and classification to identify
unlabeled data exploits the idea of the pipeline workflow discussed in the previous sections.

Let's illustrate the process by loading some sample data:

```@example tsclassifier
using Random
using TSML

Random.seed!(12345)

trdirname = joinpath(dirname(pathof(TSML)),"../data/realdatatsclassification/training")
tstdirname = joinpath(dirname(pathof(TSML)),"../data/realdatatsclassification/testing")
modeldirname = joinpath(dirname(pathof(TSML)),"../data/realdatatsclassification/model")
```

Here's the list of files for training:
```@example tsclassifier
show(readdir(trdirname) |> x->filter(y->match(r".csv",y) != nothing,x))
```

and here are the files in testing directory:
```@example tsclassifier
show(readdir(tstdirname) |> x->filter(y->match(r".csv",y) != nothing,x))
```

The files in testing directory doesn't need to be labeled but we use the labeling as
a way to validate the effectiveness of the classifier. The labels will be used as the
groundtruth during prediction/classification.

## TSClassifier

Let us now setup an instance of the `TSClassifier` and pass the arguments containing
the directory locations of files for training, testing, and modeling.

```@example tsclassifier
using TSML: TSClassifier
using TSML: fit!, transform!

tscl = TSClassifier(Dict(:trdirectory=>trdirname,
          :tstdirectory=>tstdirname,
          :modeldirectory=>modeldirname,
          :feature_range => 6:20,
          :num_trees=>20)
       )
nothing #hide
```

Time to train our `TSClassifier` to learn the mapping between extracted stats features with the 
TS type.

```@repl tsclassifier
fit!(tscl)
```

We can examine the extracted features saved by the model that is used for its training.

```@example tsclassifier
using CSV, DataFrames

mdirname = tscl.args[:modeldirectory]
modelfname=tscl.args[:juliarfmodelname]

trstatfname = joinpath(mdirname,modelfname*".csv")
res = CSV.read(trstatfname) |> DataFrame
nothing #hide
```

```@repl tsclassifier
first(res,5)
```

Let's check the accuracy of prediction with the test data using the `transform!` function.

```@repl tsclassifier
dfresults = transform!(tscl)
```
The table above shows the prediction corresponding to each filename which is the groundtruth. We can compute
the accuracy by extracting from the filename the TS type and compare it with the corresponding prediction.
Below computes the prediction accuracy:

```@example tsclassifier
prediction = dfresults[:predtype]
fnames = dfresults[:fname]
myregex = r"(?<dtype>[A-Z _ - a-z]+)(?<number>\d*).(?<ext>\w+)"
groundtruth=map(fnames) do fname
  mymatch=match(myregex,fname)
  mymatch[:dtype]
end
nothing #hide
```

```@repl tsclassifier
sum(groundtruth .== prediction) / length(groundtruth) * 100
```

Of course we need more data to split between
training and testing to improve accuracy and get a more stable measurement of performance.
