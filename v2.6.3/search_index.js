var documenterSearchIndex = {"docs":
[{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/aggregation/#Aggregation","page":"Aggregation","title":"Aggregation","text":"","category":"section"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"DateValgator is a data type that supports operation  for aggregation to minimize noise and  lessen the occurrence of missing data. It expects to receive one argument which is the date-time interval for grouping values by taking  their median. For example, hourly median as the basis of aggregation can be carried out by passing this argument: :dateinterval => Dates.Hour(1)","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"To illustrate DateValgator usage, let's start by  generating an artificial data with sample frequencey every 5 minutes and print the first 10 rows.","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"using TSML\n\ngdate = DateTime(2014,1,1):Dates.Minute(5):DateTime(2014,5,1)\ngval = rand(length(gdate))\ndf = DataFrame(Date=gdate,Value=gval)\nnothing #hide","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"first(df,10)","category":"page"},{"location":"man/aggregation/#DateValgator","page":"Aggregation","title":"DateValgator","text":"","category":"section"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"Let's apply the aggregator and try diffent groupings: hourly vs half hourly vs daily aggregates of the data.","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"using TSML\n\nhourlyagg = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\nhalfhourlyagg = DateValgator(Dict(:dateinterval => Dates.Minute(30)))\ndailyagg = DateValgator(Dict(:dateinterval => Dates.Day(1)))\n\nhalfhourlyres = fit_transform!(halfhourlyagg,df)\n\nhourlyres = fit_transform!(hourlyagg,df)\n\ndailyres = fit_transform!(dailyagg,df)\nnothing #hide","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"The first 5 rows of half-hourly, hourly, and daily aggregates:","category":"page"},{"location":"man/aggregation/","page":"Aggregation","title":"Aggregation","text":"first(halfhourlyres,5)\nfirst(hourlyres,5)\nfirst(dailyres,5)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/monotonic_plotting/#Monotonic-Detection-and-Plotting","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"One important preprocessing step for time series data processing is the detection  of monotonic data and transform it to non-monotonic type by using the finite difference operator.","category":"page"},{"location":"tutorial/monotonic_plotting/#Artificial-Data-Example","page":"Monotonic Detection and Plotting","title":"Artificial Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's create an artificial monotonic data and apply our monotonic transformer to normalize it. We can use the Plotter filter to visualize the generated data.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nRandom.seed!(123)\npltr = Plotter(Dict(:interactive => false,:pdfoutput => false))\nmdates = DateTime(2017,12,1,1):Dates.Hour(1):DateTime(2017,12,31,10) |> collect\nmvals = rand(length(mdates)) |> cumsum\ndf =  DataFrame(Date=mdates ,Value = mvals)\nfit_transform!(pltr,df)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Now that we have a monotonic data, let's use the Monotonicer to normalize and plot the result:","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nmono = Monotonicer(Dict())\n\npipeline = @pipeline mono |> pltr\n\nres=fit_transform!(pipeline,df)","category":"page"},{"location":"tutorial/monotonic_plotting/#Real-Data-Example","page":"Monotonic Detection and Plotting","title":"Real Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We will now apply the entire pipeline  starting from reading csv data, aggregate, impute, normalize if it's monotonic, and plot. We will consider three  different data types: a regular time series data, a   monotonic data, and a daily monotonic data. The difference between   monotonic and daily monotonic is that the values in daily monotonic resets to  zero or some baseline and cumulatively increases in a day until the  next day where it resets to zero or some baseline value. Monotonicer automatically detects these three different types and apply the corresponding normalization accordingly.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nregularfile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/regular.csv\")\nmonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/monotonic.csv\")\ndailymonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/dailymonotonic.csv\")\n\nregularfilecsv = CSVDateValReader(Dict(:filename=>regularfile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nmonofilecsv = CSVDateValReader(Dict(:filename=>monofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\ndailymonofilecsv = CSVDateValReader(Dict(:filename=>dailymonofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\n\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\npltr = Plotter(Dict(:interactive => false))\nnothing #hide","category":"page"},{"location":"tutorial/monotonic_plotting/#Regular-TS-Processing","page":"Monotonic Detection and Plotting","title":"Regular TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's test by feeding the regular time series type to the pipeline. We expect that for this type, Monotonicer will not perform further processing:","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline regularfilecsv |> valgator |> valnner |> mono |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline regularfilecsv |> valgator |> valnner |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that the plots are the same with or without the Monotonicer instance.","category":"page"},{"location":"tutorial/monotonic_plotting/#Monotonic-TS-Processing","page":"Monotonic Detection and Plotting","title":"Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's now feed the same pipeline with a monotonic csv data.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline monofilecsv |> valgator |> valnner |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline monofilecsv |> valgator |> valnner |> mono |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that without the Monotonicer instance, the data is monotonic. Applying the Monotonicer instance in the pipeline converts the data into a regular time series but with outliers.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We can use the Outliernicer filter to remove outliers. Let's apply this filter after the Monotonicer and plot the result.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML: Outliernicer\noutliernicer = Outliernicer(Dict(:dateinterval=>Dates.Hour(1)));\n\npipeline = @pipeline monofilecsv |> valgator |> valnner |> mono |>  outliernicer |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/#Daily-Monotonic-TS-Processing","page":"Monotonic Detection and Plotting","title":"Daily Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Lastly, let's feed the daily monotonic data using similar pipeline and examine its plot.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"This plot is characterized by monotonically increasing trend but resets to certain baseline value  at the end of the day and repeat similar trend daily. The challenge for the monotonic normalizer is to differentiate between daily monotonic from the typical monotonic function to apply the correct normalization.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> mono |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"While the Monotonicer filter is able to transform the data into a regular time series, there are significant outliers due to noise and the nature of this kind of data or sensor.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's remove the outliers by applying the Outliernicer filter and examine the result.","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> mono |> outliernicer |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"The Outliernicer filter effectively removed the outliers as shown in the plot.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/statistics/#Statistical-Metrics","page":"Statistical Metrics","title":"Statistical Metrics","text":"","category":"section"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Each TS can be evaluated to extract its statistical features which can be used for data quality assessment, data discovery by clustering and classification, and anomaly characterization among others.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"TSML relies on Statifier to perform statistical metrics on the TS which can be configured to extract the statistics of missing blocks aside from the non-missing elements. Some of the scalar statistics it uses include: pacf, acf, autocor, quartiles, mean, median, max, min, kurtosis, skewness, variation, standard error, entropy, etc. It has only one argument :processmissing => true which indicates whether to include the statistics of missing data.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us again start generating an artificial data with missing values  using the generateDataWithMissing() described in the beginning of tutorial.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"using TSML\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[!,:Value][gndxmissing] .= missing\n   return df\nend","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"X = generateDataWithMissing();\nfirst(X,15)","category":"page"},{"location":"tutorial/statistics/#Statifier-for-Both-Non-Missing-and-Missing-Values","page":"Statistical Metrics","title":"Statifier for Both Non-Missing and Missing Values","text":"","category":"section"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"TSML includes Statifier transformer that computes scalar statistics to characterize the time series data. By default, it also computes statistics of  missing blocks of data. To disable this feature, one can pass  :processmissing => false to the argument during its instance creation. Below illustrates this workflow.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"using TSML\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\ndtvalizer = DateValizer(Dict(:dateinterval => Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing => true))\n\nmypipeline = @pipeline dtvalgator |> stfier\n\nresults = fit_transform!(mypipeline,X)\nnothing #hide","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"show(results,allcols=true)","category":"page"},{"location":"tutorial/statistics/#Statifier-for-Non-Missing-Values-only","page":"Statistical Metrics","title":"Statifier for Non-Missing Values only","text":"","category":"section"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"If you are not intested with the statistics of the missing blocks, you can disable missing blocks stat summary by indicating :processmissing => false in the instance argument:","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>false))\n\nmypipeline = @pipeline dtvalgator |> stfier\n\nresults = fit_transform!(mypipeline,X)\nnothing #hide","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"show(results,allcols=true)","category":"page"},{"location":"tutorial/statistics/#Statifier-After-Imputation","page":"Statistical Metrics","title":"Statifier After Imputation","text":"","category":"section"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us check the statistics after the imputation by adding DateValNNer instance in the pipeline. We expect that if the imputation is successful, the stats for missing blocks will all be NaN because stats of empty set is an NaN.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\n\nmypipeline = @pipeline dtvalgator |> dtvalnner |> stfier\n\nresults = fit_transform!(mypipeline,X)\nnothing #hide","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"show(results,allcols=true)","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"As we expected, the imputation is successful and there are no more missing values in the processed time series dataset.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let's try with the other imputation using DateValizer and validate that there are no more missing values based on the stats.","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\n\nmypipeline = @pipeline dtvalgator |> dtvalizer |> stfier\n\nresults = fit_transform!(mypipeline,X)\nnothing #hide","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"show(results,allcols=true)","category":"page"},{"location":"tutorial/statistics/","page":"Statistical Metrics","title":"Statistical Metrics","text":"Indeed, the imputation got rid of the missing values.","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/imputation/#Imputation","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"There are two ways to impute the date,value TS data. One uses DateValNNer which uses nearest neighbor and DateValizer which uses the dictionary of medians mapped to  certain date-time interval grouping.","category":"page"},{"location":"man/imputation/#DateValNNer","page":"Imputation","title":"DateValNNer","text":"","category":"section"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"DateValNNer expects the following arguments with their default values during instantation: ","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":":dateinterval => Dates.Hour(1)  \ngrouping interval\n:nnsize => 1 \nsize of neighborhood\n:missdirection => :symmetric \n:forward vs :backward vs :symmetric\n:strict => true \nwhether or not to repeatedly iterate until no more missing data","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"The :missdirection indicates the imputation direction and the extent of neighborhood. Symmetric implies getting info from both sides of the missing data. :forward direction starts imputing from the top while the :reverse starts from the bottom. Please refer to  Aggregators and Imputers for other examples.","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"using TSML\n\nfunction generateXY()\n    Random.seed!(123)\n    gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n    gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n    gmissing = 50000\n    gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n    X = DataFrame(Date=gdate,Value=gval)\n    X[!,:Value][gndxmissing] .= missing\n    Y = rand(length(gdate))\n    (X,Y)\nend\nX,Y = generateXY()","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Let's use the same dataset we have used in the tutorial and print the first few rows.","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"first(X,10)","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Let's try the following setup grouping daily with forward imputation and 10 neighbors:","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :forward,\n             :strict=>false))\nforwardres=fit_transform!(dnnr,X)\nnothing #hide","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"first(forwardres,5)","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Same parameters as above but uses reverse instead of forward direction:","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :reverse,\n             :strict=>false))\nreverseres=fit_transform!(dnnr,X)\nnothing #hide","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"first(reverseres,5)","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Using symmetric imputation:","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"dnnr = DateValNNer(Dict(:dateinterval=>Dates.Hour(2),\n             :nnsize=>10,:missdirection => :symmetric,\n             :strict=>false))\nsymmetricres=fit_transform!(dnnr,X)\nnothing #hide","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"first(symmetricres,5)","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Unlike symmetric imputation that guarantees 100% imputation of missing data as long as the input has non-missing elements, forward and reverse cannot guarantee that the imputation replaces all missing data because of the boundary issues. If the top or bottom of the input is missing, the assymetric imputation will not be able to replace the endpoints that are missing. It is advised that to have successful imputation, symmetric imputation shall be used.","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"In the example above, the number of remaining missing data not imputed for forward, reverse, and symmetric is:","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"sum(ismissing.(forwardres.Value))\nsum(ismissing.(reverseres.Value))\nsum(ismissing.(symmetricres.Value))","category":"page"},{"location":"man/imputation/#DateValizer","page":"Imputation","title":"DateValizer","text":"","category":"section"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"DateValizer operates on the principle that there is a reqularity of patterns in a specific time period such that replacing values is just a matter of  extracting which time period it belongs and used the pooled median in that time period to replace the missing data. The default time period for DateValizer is hourly. In a more advanced implementation, we can add daily, hourly, and weekly  periods but it will require much larger hash table. Additional grouping criteria  can result into smaller subgroups which may contain 100% missing in some of these subgroups resulting to imputation failure. DateValizer only depends on the :dateinterval => Dates.Hour(1)  argument with default value of hourly. Please refer to Aggregators and Imputers for more examples.","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"Let's try hourly, daily, and monthly median as the basis of imputation:","category":"page"},{"location":"man/imputation/","page":"Imputation","title":"Imputation","text":"hourlyzer = DateValizer(Dict(:dateinterval => Dates.Hour(1)));\nmonthlyzer = DateValizer(Dict(:dateinterval => Dates.Month(1)));\ndailyzer = DateValizer(Dict(:dateinterval => Dates.Day(1)));\n\nhourlyres = fit_transform!(hourlyzer,X)\n\ndailyres = fit_transform!(dailyzer,X)\n\nmonthlyres = fit_transform!(monthlyzer,X)","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/aggregators/#aggregators_imputers","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"","category":"section"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The package assumes a two-column table composed of Dates and Values.  The first part of the workflow aggregates values based on the specified  date-time interval which minimizes occurence of missing values and noise.  The aggregated data is then left-joined to the complete sequence of  DateTime  in a specified date-time interval. Remaining missing values are replaced  by k nearest neighbors where k is the symmetric distance from the location  of missing value. This replacement algo is called several times until there  are no more missing values.","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let us create a Date, Value table with some missing values and output the first 15 rows. We will then apply some TSML functions to normalize/clean the data. Below is the code of the generateDataWithMissing() function:","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[!,:Value][gndxmissing] .= missing\n   return df\nend\nnothing #hide","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"X = generateDataWithMissing();\nfirst(X,15)","category":"page"},{"location":"tutorial/aggregators/#DateValgator","page":"Aggregators and Imputers","title":"DateValgator","text":"","category":"section"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"You'll notice several blocks of missing in the table above with reading frequency of every 15 minutes.  To minimize noise and lessen the occurrence of missing values, let's aggregate our dataset by taking the hourly median using the DateValgator transformer.","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\n\ndtvlgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nresults = fit_transform!(dtvlgator,X)\nnothing #hide","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"first(results,10)","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The occurrence of missing values is now reduced because of the hourly aggregation. While the default is hourly aggregation, you can easily change it by using a different interval in the argument during instance creation. Below indicates every 30 minutes interval.","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"dtvlgator = DateValgator(Dict(:dateinterval=>Dates.Minute(30)))","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"DateValgator is one of the several TSML transformers to preprocess and clean the  time series data. In order to create additional transformers to extend TSML,  each transformer must overload the two Transformer functions:fit! and transform!.  DateValgator fit! performs initial setups of necessary parameters and validation of arguments while its transform! function contains the algorithm  for aggregation. ","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"For machine learning prediction and classification transformer,  fit! function is equivalent to ML training or parameter optimization,  while the transform! function is for doing the actual prediction. The later part of the tutorial will provide an example how to add a Transformer to extend the functionality of TSML.","category":"page"},{"location":"tutorial/aggregators/#DateValNNer","page":"Aggregators and Imputers","title":"DateValNNer","text":"","category":"section"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let's perform further processing to replace the remaining missing values with their nearest neighbors.  We will use DateValNNer which is a TSML transformer to process the output of DateValgator. DateValNNer can also process non-aggregated data by first running similar workflow of DateValgator before performing its imputation routine.","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\n\ndatevalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nresults = fit_transform!(datevalnner,X)\nnothing #hide","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"first(results,10)","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"After running the DateValNNer, it's guaranteed that there will be no more missing data unless the input are all missing data.","category":"page"},{"location":"tutorial/aggregators/#DateValizer","page":"Aggregators and Imputers","title":"DateValizer","text":"","category":"section"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"One more imputer to replace missing data is DateValizer. It computes the hourly median over 24 hours and use the hour => median hashmap learned to replace missing data using hour as the key. In this implementation, fit! function is doing the training of parameters by computing the medians and save it for the transform! function to use for imputation. It is possible that the hashmap can contain missing values in cases where the pooled hourly median in a particular hour have all missing data. Below is a sample workflow to replace missing data in X with the hourly medians.","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\n\ndatevalizer = DateValizer(Dict(:dateinterval=>Dates.Hour(1)))\nresults = fit_transform!(datevalizer,X)\nnothing #hide","category":"page"},{"location":"tutorial/aggregators/","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"first(results,10)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/pipeline/#Pipeline","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Instead of calling fit! and transform! for each transformer to process time series data, we can use the Pipeline transformer which does this automatically by iterating through the transformers and calling fit! and transform! repeatedly for each transformer in its argument.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let's start again by using a function to generate a time series dataframe with some missing data.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using TSML\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[!,:Value][gndxmissing] .= missing\n   return df\nend","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"X = generateDataWithMissing();\nfirst(X,15)","category":"page"},{"location":"tutorial/pipeline/#Workflow-of-Pipeline","page":"Pipeline","title":"Workflow of Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let's use the pipeline transformer to aggregate and impute:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using TSML\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\n\nmypipeline = @pipeline dtvalgator |> dtvalnner\n\nresults = fit_transform!(mypipeline,X)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"first(results,10)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Using the Pipeline transformer, it becomes straightforward to process the time series data. It also becomes trivial to extend TSML functionality by adding more transformers and making sure each support the fit! and transform! interfaces. Any new transformer can then be easily added to the Pipeline workflow  without invasively changing the existing codes.","category":"page"},{"location":"tutorial/pipeline/#Extending-TSML","page":"Pipeline","title":"Extending TSML","text":"","category":"section"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"To illustrate how simple it is to add a new transformer, below extends TSML by adding CSVReader transformer and added in the pipeline to process CSV data:","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"using TSML\nimport TSML.AbsTypes.fit!\nimport TSML.AbsTypes.transform!\n\nmutable struct CSVReader <: Transformer\n    filename::String\n    model::Dict{Symbol,Any}\n\n    function CSVReader(args=Dict())\n        default_args = Dict(\n            :filename => \"\",\n            :dateformat => \"\"\n        )\n        margs = nested_dict_merge(default_args, args)\n        new(margs[:filename],margs)\n    end\nend\n\nfunction fit!(csvrdr::CSVReader,x::DataFrame=DataFrame(),y::Vector=[]) \n    fname = csvrdr.model[:filename]\n    fmt = csvrdr.model[:dateformat]\n    (fname != \"\" && fmt != \"\") || error(\"missing filename or date format\")\nend\n\nfunction transform!(csvrdr::CSVReader,x::DataFrame=DataFrame())\n    fname = csvrdr.model[:filename]\n    fmt = csvrdr.model[:dateformat]\n    df = CSV.read(fname)\n    ncol(df) == 2 || error(\"dataframe should have only two columns: Date,Value\")\n    rename!(df,names(df)[1]=>:Date,names(df)[2]=>:Value)\n    df.Date = DateTime.(df.Date,fmt)\n    df\nend\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Instead of passing table X that contains the time series, we will add  an instance of the CSVReader at the start of the array of transformers in the pipeline  to read the csv data. CSVReader transform! function converts the csv time series table into a dataframe, which will be consumed by the next transformer in the pipeline  for processing.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"fname = joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvreader = CSVDateValReader(Dict(:filename=>fname,:dateformat=>\"d/m/y H:M\"))\nfit!(csvreader)\ncsvdata = transform!(csvreader)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"first(csvdata,10)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Let us now include the newly created CSVReader in the pipeline to read the csv data and process it by aggregation and imputation.","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"mypipeline = @pipeline csvreader |> dtvalgator |> dtvalnner\n\nresults = fit_transform!(mypipeline)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"first(results,10)","category":"page"},{"location":"tutorial/pipeline/","page":"Pipeline","title":"Pipeline","text":"Notice that there is no more the need to pass X in the arguments of fit! and transform because the data is now transmitted by the CSVReader instance to the other transformers in the pipeline.","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/valueproc/#valueprep","page":"Value PreProcessing","title":"Value Preprocessing","text":"","category":"section"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"In order to process 1-D TS as input for ML model, it has to be converted into Matrix form where each row represents a  slice of 1-D TS representing daily/hourly/weekly pattern depending on the size of the chunk, stride, and  number of steps ahead for prediction. Below illustrates the processing workflow to Matrify a 1-D TS.","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"For illustration purposes, the code below generates a  Date,Value dataframe where the values are just a sequece of integer from 1 to the length of the date sequence. We use this simple sequence to have a better understanding how the slicing of rows, steps ahead, and the stride to create the Matrified output is generated.","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"using TSML\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2017,1,5)\ndat=lower:Dates.Hour(1):upper |> collect\nvals = 1:length(dat)\nx = DataFrame(Date=dat,Value=vals)\nnothing #hide","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"last(x,5)","category":"page"},{"location":"man/valueproc/#Matrifier","page":"Value PreProcessing","title":"Matrifier","text":"","category":"section"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"Let us create an instance of Matrifier passing the size of row, stride, and steps ahead to predict:","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"mtr = Matrifier(Dict(:ahead=>6,:size=>6,:stride=>3))\nres = fit_transform!(mtr,x)\nnothing #hide","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"first(res,5)","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"In this example, we have hourly values. We indicated in the  Matrifier to generate a matrix where the size of each row is 6 hours, steps ahead for prediction is 6 hours and the stride of 3 hours. There are 7 columns because the last column indicates the value indicated by the steps ahead argument.","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"Let us try to make a matrix with the size of 6 hours, steps ahead of 2 hours, and a stride of 3 hours:","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"mtr = Matrifier(Dict(:ahead=>2,:size=>6,:stride=>3))\nres = fit_transform!(mtr,x)\nnothing #hide","category":"page"},{"location":"man/valueproc/","page":"Value PreProcessing","title":"Value PreProcessing","text":"first(res,5)","category":"page"},{"location":"lib/functions/","page":"Types and Functions","title":"Types and Functions","text":"Author = \"Paulito Palmes\"","category":"page"},{"location":"lib/functions/#lib_decisiontree","page":"Types and Functions","title":"Types and Functions","text":"","category":"section"},{"location":"lib/functions/#Index","page":"Types and Functions","title":"Index","text":"","category":"section"},{"location":"lib/functions/","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   TSML.ArgumentParsers,\n   TSML.BaseFilters,\n   TSML.BaselineModels, \n   TSML.CLIWrappers,\n   TSML.CrossValidators,\n   TSML.DecisionTreeLearners,\n   TSML.EnsembleMethods,\n   TSML.FeatureSelectors,\n   TSML.MLBaseWrapper,\n   TSML.Monotonicers,\n   TSML.Normalizers,\n   TSML.Outliernicers, \n   TSML.Pipelines,\n   TSML.Plotters,\n   TSML.Statifiers,\n   TSML.TSClassifiers,\n   TSML.TSMLDemo,\n   TSML.ValDateFilters,\n   ]","category":"page"},{"location":"lib/functions/#Descriptions","page":"Types and Functions","title":"Descriptions","text":"","category":"section"},{"location":"lib/functions/","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   TSML.ArgumentParsers,\n   TSML.BaseFilters,\n   TSML.BaselineModels,\n   TSML.CLIWrappers,\n   TSML.CrossValidators,\n   TSML.DecisionTreeLearners,\n   TSML.EnsembleMethods,\n   TSML.FeatureSelectors,\n   TSML.MLBaseWrapper,\n   TSML.Monotonicers,\n   TSML.Normalizers,\n   TSML.Outliernicers, \n   TSML.Pipelines,\n   TSML.Plotters,\n   TSML.Statifiers,\n   TSML.TSClassifiers,\n   TSML.TSMLDemo,\n   TSML.ValDateFilters,\n   ]","category":"page"},{"location":"lib/functions/#AMLPipelineBase.BaseFilters.Imputer","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.Imputer","text":"Imputer(\n   Dict(\n      # Imputation strategy.\n      # Statistic that takes a vector such as mean or median.\n      :strategy => mean\n   )\n)\n\nImputes NaN values from Float64 features.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.BaseFilters.OneHotEncoder","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.OneHotEncoder","text":"OneHotEncoder(Dict(\n   # Nominal columns\n   :nominal_columns => Int[],\n\n   # Nominal column values map. Key is column index, value is list of\n   # possible values for that column.\n   :nominal_column_values_map => Dict{Int,Any}()\n))\n\nTransforms myinstances with nominal features into one-hot form and coerces the instance matrix to be of element type Float64.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.BaseFilters.Wrapper","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.Wrapper","text":"Wrapper(\n   default_args = Dict(\n      :name => \"ohe-wrapper\",\n      # Transformer to call.\n      :transformer => OneHotEncoder(),\n      # Transformer args.\n      :transformer_args => Dict()\n   )\n)\n\nWraps around a transformer.\n\nImplements fit! and transform.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.BaseFilters.createtransformer","page":"Types and Functions","title":"AMLPipelineBase.BaseFilters.createtransformer","text":"createtransformer(prototype::Transformer, args=Dict())\n\nCreate transformer\n\nprototype: prototype transformer to base new transformer on\noptions: additional options to override prototype's options\n\nReturns: new transformer.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.BaselineModels.Baseline","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Baseline","text":"Baseline(\n   default_args = Dict(\n       :name => \"baseline\",\n      :output => :class,\n      :strat => mode\n   )\n)\n\nBaseline model that returns the mode during classification.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.BaselineModels.Baseline-Tuple{String}","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Baseline","text":"Baseline(name::String,opt...)\n\nHelper function\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.BaselineModels.Identity","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Identity","text":"Identity(args=Dict())\n\nReturns the input as output.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.BaselineModels.Identity-Tuple{String}","page":"Types and Functions","title":"AMLPipelineBase.BaselineModels.Identity","text":"Baseline(name::String,opt...)\n\nHelper function\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Baseline,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(bsl::Baseline,x::DataFrame,y::Vector)\n\nGet the mode of the training data.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Identity,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(idy::Identity,x::DataFrame,y::Vector)\n\nDoes nothing.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Baseline,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(bsl::Baseline,x::DataFrame)\n\nReturn the mode in classification.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Identity,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(idy::Identity,x::DataFrame)\n\nReturn the input as output.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.CrossValidators.crossvalidate-Tuple{Machine,DataFrame,Array{T,1} where T,Function,Int64,Bool}","page":"Types and Functions","title":"AMLPipelineBase.CrossValidators.crossvalidate","text":"crossvalidate(pl::Machine,X::DataFrame,Y::Vector,pfunc::Function,kfolds=10)\n\nRun K-fold crossvalidation where:\n\npfunc is a performance metric\nX and Y are input and target \n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.DecisionTreeLearners.Adaboost","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.Adaboost","text":"Adaboost(\n  Dict(\n    :output => :class,\n    :num_iterations => 7\n  )\n)\n\nAdaboosted decision tree stumps. See DecisionTree.jl's documentation\n\nHyperparameters:\n\n:num_iterations => 7 (number of iterations of AdaBoost)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.DecisionTreeLearners.PrunedTree","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.PrunedTree","text":"PrunedTree(\n  Dict(\n    :purity_threshold => 1.0,\n    :max_depth => -1,\n    :min_samples_leaf => 1,\n    :min_samples_split => 2,\n    :min_purity_increase => 0.0\n  )\n)\n\nDecision tree classifier.   See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:purity_threshold => 1.0 (merge leaves having >=thresh combined purity)\n:max_depth => -1 (maximum depth of the decision tree)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.DecisionTreeLearners.RandomForest","page":"Types and Functions","title":"AMLPipelineBase.DecisionTreeLearners.RandomForest","text":"RandomForest(\n  Dict(\n    :output => :class,\n    :num_subfeatures => 0,\n    :num_trees => 10,\n    :partial_sampling => 0.7,\n    :max_depth => -1\n  )\n)\n\nRandom forest classification.  See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:num_subfeatures => 0  (number of features to consider at random per split)\n:num_trees => 10 (number of trees to train)\n:partial_sampling => 0.7 (fraction of samples to train each tree on)\n:max_depth => -1 (maximum depth of the decision trees)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{Adaboost,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(adaboost::Adaboost, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{PrunedTree,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(tree::PrunedTree, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{RandomForest,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(forest::RandomForest, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nOptimize the parameters of the RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Adaboost,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(adaboost::Adaboost, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{PrunedTree,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(ptree::PrunedTree, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{RandomForest,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(forest::RandomForest, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.EnsembleMethods.BestLearner","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.BestLearner","text":"BestLearner(\n   Dict(\n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Function to return partitions of instance indices.\n      :partition_generator => (instances, labels) -> kfold(size(instances, 1), 5),\n      # Function that selects the best learner by index.\n      # Arg learner_partition_scores is a (learner, partition) score matrix.\n      :selection_function => (learner_partition_scores) -> findmax(mean(learner_partition_scores, dims=2))[2],      \n      # Score type returned by score() using respective output.\n      :score_type => Real,\n      # Candidate learners.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Options grid for learners, to search through by BestLearner.\n      # Format is [learner_1_options, learner_2_options, ...]\n      # where learner_options is same as a learner's options but\n      # with a list of values instead of scalar.\n      :learner_options_grid => nothing\n   )\n)\n\nSelects best learner from the set by performing a  grid search on learners if grid option is indicated.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.EnsembleMethods.StackEnsemble","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.StackEnsemble","text":"StackEnsemble(\n   Dict(    \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Set of learners that produce feature space for stacker.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()],\n      # Machine learner that trains on set of learners' outputs.\n      :stacker => RandomForest(),\n      # Proportion of training set left to train stacker itself.\n      :stacker_training_proportion => 0.3,\n      # Provide original features on top of learner outputs to stacker.\n      :keep_original_features => false\n   )\n)\n\nAn ensemble where a 'stack' of learners is used for training and prediction.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.EnsembleMethods.VoteEnsemble","page":"Types and Functions","title":"AMLPipelineBase.EnsembleMethods.VoteEnsemble","text":"VoteEnsemble(\n   Dict( \n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Learners in voting committee.\n      :learners => [PrunedTree(), Adaboost(), RandomForest()]\n   )\n)\n\nSet of machine learners employing majority vote to decide prediction.\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{BestLearner,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(bls::BestLearner, instances::DataFrame, labels::Vector)\n\nTraining phase:\n\nobtain learners as is if grid option is not present \ngenerate learners if grid option is present \nforeach prototype learner, generate learners with specific options found in grid\ngenerate partitions\ntrain each learner on each partition and obtain validation output\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{StackEnsemble,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(se::StackEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the stack of learners.\n\nperform holdout to obtain indices for \npartition learner and stacker training sets\npartition training set for learners and stacker\ntrain all learners\ntrain stacker on learners' outputs\nbuild final model from the trained learners\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-Tuple{VoteEnsemble,DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(ve::VoteEnsemble, instances::DataFrame, labels::Vector)\n\nTraining phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{BestLearner,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(bls::BestLearner, instances::DataFrame)\n\nChoose the best learner based on cross-validation results and use it for prediction.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{StackEnsemble,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(se::StackEnsemble, instances::DataFrame)\n\nBuild stacker instances and predict\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{VoteEnsemble,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(ve::VoteEnsemble, instances::DataFrame)\n\nPrediction phase of the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.CatFeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatFeatureSelector","text":"CatFeatureSelector(Dict(:name => \"catf\"))\n\nAutomatically extract categorical columns based on  inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","text":"CatNumDiscriminator(\n   Dict(\n      :name => \"catnumdisc\",\n      :maxcategories => 24\n   )\n)\n\nTransform numeric columns to string (as categories)  if the count of their unique elements <= maxcategories.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.CatNumDiscriminator-Tuple{Int64}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.CatNumDiscriminator","text":"CatNumDiscriminator(maxcat::Int)\n\nHelper function for CatNumDiscriminator.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.FeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(\n   Dict(\n     :name => \"featureselector\",\n     :columns => [col1, col2, ...]\n   )\n)\n\nReturns a dataframe of the selected columns.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.FeatureSelector-Tuple{Array{Int64,1}}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(cols::Vector{Int})\n\nHelper function for FeatureSelector.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.FeatureSelector-Tuple{Vararg{Int64,N} where N}","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.FeatureSelector","text":"FeatureSelector(cols::Vararg{Int})\n\nHelper function for FeatureSelector.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.FeatureSelectors.NumFeatureSelector","page":"Types and Functions","title":"AMLPipelineBase.FeatureSelectors.NumFeatureSelector","text":"NumFeatureSelector(Dict(:name=>\"numfeatsel\"))\n\nAutomatically extracts numeric features based on their inferred element types.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.MLBaseWrapper.StandardScaler","page":"Types and Functions","title":"TSML.MLBaseWrapper.StandardScaler","text":"StandardScaler(\n   Dict( \n      :impl_args => Dict(\n          :center => true,\n          :scale => true\n      )\n   )\n)\n\nStandardizes each feature using (X - mean) / stddev. Will produce NaN if standard deviation is zero.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.MLBaseWrapper.Standardize","page":"Types and Functions","title":"TSML.MLBaseWrapper.Standardize","text":"Standardize(d::Int, m::Vector{Float64}, s::Vector{Float64})\n\nStandardization type.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(st::StandardScaler, features::T, labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nCompute the parameters to center and scale.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{StandardScaler,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(st::StandardScaler, features::T)  where {T<:Union{Vector,Matrix,DataFrame}}\n\nApply the computed parameters for centering and scaling to new data.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#TSML.Monotonicers.Monotonicer","page":"Types and Functions","title":"TSML.Monotonicers.Monotonicer","text":"Monotonicer()\n\nMonotonic filter to detect and normalize two types of dataset: \n\ndaily monotonic \nentirely non-decreasing/non-increasing data\n\nExample: \n\nfname = joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvfilter = CSVDateValReader(Dict(:filename=>fname,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\n\nmypipeline = @pipeline csvfilter |> valgator |> mono |> stfier\nresult = fit_transform!(mypipeline)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-2","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(st::Monotonicer,features::T, labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nA function that checks if features are two-column data of  Dates and Values\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Monotonicer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(st::Monotonicer, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nNormalize monotonic or daily monotonic data by taking the diffs and counting the flips.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#TSML.Normalizers.Normalizer","page":"Types and Functions","title":"TSML.Normalizers.Normalizer","text":"Normalizer(Dict(\n   :method => :zscore\n))\n\nTransforms continuous features into normalized form such as zscore, unitrange, square-root, log, pca, ppca with parameter: \n\n:method => :zscore or :unitrange or :sqrt or :log or pca or ppca or fa\n:zscore => standard z-score with centering and scaling\n:unitrange => unit range normalization with centering and scaling\n:sqrt => square-root transform\n:pca => principal component analysis transform\n:ppca => probabilistic pca\n:fa => factor analysis\n:log => log transform\n\nExample:\n\nfunction generatedf()\n    Random.seed!(123)\n    gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n    gval1 = rand(length(gdate))\n    gval2 = rand(length(gdate))\n    gval3 = rand(length(gdate))\n    X = DataFrame(Date=gdate,Value1=gval1,Value2=gval2,Value3=gval3)\n    X\nend\n\nX = generatedf()\nnorm = Normalizer(Dict(:method => :zscore))\nfit!(norm,X)\nres=transform!(norm,X)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-3","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(st::Statifier, features::T, labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nValidate argument features other than dates are continuous.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Normalizer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(norm::Normalizer, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nCompute statistics.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#TSML.Outliernicers.Outliernicer","page":"Types and Functions","title":"TSML.Outliernicers.Outliernicer","text":"Outliernicer(Dict(\n   :dateinterval => Dates.Hour(1),\n   :nnsize => 1,\n   :missdirection => :symmetric,\n   :scale => 1.25\n))\n\nDetects outliers below or above (median-scaleiqr,median+scaleiqr) and calls DateValNNer to replace them with nearest neighbors.\n\nExample:\n\nfname = joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvfilter = CSVDateValReader(Dict(:filename=>fname,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\noutliernicer = Outliernicer(Dict(:dateinterval=>Dates.Hour(1)))\n\nmpipeline = @pipeline csvfilter |> valgator |> mono |> valnner |> outliernicer |> stfier\nresults = fit_transform!(mpipeline)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-4","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(st::Outliernicer, features::T, labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nCheck that features are two-colum data.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Outliernicer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(st::Outliernicer, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nLocate outliers based on IQR factor and calls DateValNNer to replace them with nearest neighbors.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.Pipelines.ComboPipeline","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.ComboPipeline","text":"ComboPipeline(machs::Vector{T}) where {T<:Machine}\n\nFeature union pipeline which iteratively calls  fit_transform of each element and concatenate their output into one dataframe.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.Pipelines.Pipeline","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vector{<:Machine},args::Dict=Dict())\n\nLinear pipeline which iteratively calls and passes the result of fit_transform to the succeeding elements in the pipeline.\n\nImplements fit! and transform!.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.Pipelines.Pipeline-Tuple{Array{var\"#s20\",1} where var\"#s20\"<:Machine,Dict}","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vector{<:Machine},args::Dict=Dict())\n\nHelper function for Pipeline structure.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.Pipelines.Pipeline-Tuple{Vararg{Machine,N} where N}","page":"Types and Functions","title":"AMLPipelineBase.Pipelines.Pipeline","text":"Pipeline(machs::Vararg{Machine})\n\nHelper function for Pipeline structure.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#TSML.Plotters.Plotter","page":"Types and Functions","title":"TSML.Plotters.Plotter","text":"Plotter(   Dict(     :interactive => false,     :pdfoutput => false   ) )\n\nPlots a TS by default but performs interactive plotting if specified during instance creation.\n\n:interactive => boolean to indicate whether to use interactive plotting with false as default\n:pdfoutput => boolean to indicate whether ouput will be saved as pdf with false as default\n\nExample:\n\ncsvfilter = CSVDateValReader(Dict(:filename=>fname,:dateformat=>\"dd/mm/yyyy HH:MM\")) pltr = Plotter(Dict(:interactive => false))\n\nmpipeline = @pipeline csvfilter |> pltr myplot = fit_transform!(mpipeline)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-5","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(pltr::Plotter, features::T, labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nCheck validity of features: 2-column Date,Val data\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Plotter,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(pltr::Plotter, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nConvert missing into NaN to allow plotting of discontinuities.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#TSML.Statifiers.Statifier","page":"Types and Functions","title":"TSML.Statifiers.Statifier","text":"Statifier(Dict(\n   :processmissing => true\n))\n\nOutputs summary statistics such as mean, median, quartile, entropy, kurtosis, skewness, etc. with parameter: \n\n:processmissing => boolean to indicate whether to include missing data stats.\n\nExample:\n\ndt=[missing;rand(1:10,3);missing;missing;missing;rand(1:5,3)]\ndat = DataFrame(Date= DateTime(2017,12,31,1):Dates.Hour(1):DateTime(2017,12,31,10) |> collect,\n                Value = dt)\n\nstatfier = Statifier(Dict(:processmissing=>false))\n\nfit!(statfier,dat)\nresults=transform!(statfier,dat)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-6","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(st::Statifier, features::T=[], labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nValidate argument to make sure it's a 2-column format.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(st::Statifier, features::T=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nCompute statistics.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#TSML.TSClassifiers.TSClassifier","page":"Types and Functions","title":"TSML.TSClassifiers.TSClassifier","text":"TSClassifier(\n   Dict(\n      # training directory\n      :trdirectory => \"\",\n      :tstdirectory => \"\",\n      :modeldirectory => \"\",\n      :feature_range => 7:20,\n      :juliarfmodelname => \"juliarfmodel.serialized\",\n      # Output to train against\n      # (:class).\n      :output => :class,\n      # Options specific to this implementation.\n      :impl_args => Dict(\n         # Merge leaves having >= purity_threshold CombineMLd purity.\n         :purity_threshold => 1.0,\n         # Maximum depth of the decision tree (default: no maximum).\n         :max_depth => -1,\n         # Minimum number of samples each leaf needs to have.\n         :min_samples_leaf => 1,\n         # Minimum number of samples in needed for a split.\n         :min_samples_split => 2,\n         # Minimum purity needed for a split.\n         :min_purity_increase => 0.0\n      )\n   )\n)\n\nGiven a bunch of time-series with specific types. Get the statistical features of each, use these as inputs to RF classifier with output as the TS type, train and test. Another option is to use these stat features for clustering and check cluster quality. If accuracy is poor, add more stat features and repeat same process as outlined for training and testing. Assume that each time-series is named based on their type which will be used as target output. For example, temperature time series will be named as temperature?.csv where ? is an integer. Loop over each file in a directory, get stat and  record in a dictionary/dataframe, train/test. Default to using RandomForest  for classification of data types.\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-7","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(tsc::TSClassifier, features::T=[], labels::Vector=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nGet the stats of each file, collect as dataframe, and train.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-2","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(tsc::TSClassifier, features::T=[]) where {T<:Union{Vector,Matrix,DataFrame}}\n\nApply the learned parameters to the new data.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#TSML.ValDateFilters.CSVDateValReader","page":"Types and Functions","title":"TSML.ValDateFilters.CSVDateValReader","text":"CSVDateValReader(\n   Dict(\n      :filename => \"\",\n      :dateformat => \"\"\n   )\n)\n\nReads csv file and parse date using the given format.\n\n:filename => complete path including filename of csv file\n:dateformat => date format to parse\n\nExample:\n\ninputfile =joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvreader = CSVDateValReader(Dict(:filename=>inputfile,:dateformat=>\"d/m/y H:M\"))\nfit!(csvreader)\ndf = transform!(csvreader)\n\n# using pipeline workflow\nfilter1 = DateValgator()\nfilter2 = DateValNNer(Dict(:nnsize=>1))\nmypipeline = @pipeline csvreader |> filter1 |> filter2\nfit!(mypipeline)\nres=transform!(mypipeline)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.CSVDateValWriter","page":"Types and Functions","title":"TSML.ValDateFilters.CSVDateValWriter","text":"CSVDateValWriter(\n   Dict(\n      :filename => \"\",\n      :dateformat => \"\"\n   )\n)\n\nWrites the time series dataframe into a file with the given date format.\n\nExample:\n\ninputfile =joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\noutputfile = joinpath(\"/tmp/test.csv\")\ncsvreader = CSVDateValReader(Dict(:filename=>inputfile,:dateformat=>\"d/m/y H:M\"))\ncsvwtr = CSVDateValWriter(Dict(:filename=>outputfile,:dateformat=>\"d/m/y H:M\"))\nfilter1 = DateValgator()\nfilter2 = DateValNNer(Dict(:nnsize=>1))\nmypipeline = @pipeline csvreader |> filter1 |> filter2 |> csvwtr\nres=fit_transform!(mypipeline)\n\n# read back what was written to validate\ncsvreader = CSVDateValReader(Dict(:filename=>outputfile,:dateformat=>\"y-m-d HH:MM:SS\"))\nfit!(csvreader)\ntransform!(csvreader)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.DateValLinearImputer","page":"Types and Functions","title":"TSML.ValDateFilters.DateValLinearImputer","text":"DateValLinearImputer(\n   Dict(\n      :dateinterval => Dates.Hour(1),\n  )\n)\n\nFills missings by linear interpolation.\n\n:dateinterval => time period to use for grouping,\n\nExample:\n\nRandom.seed!(123)\ngdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\ngval = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngmissing = 50000\ngndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\nX = DataFrame(Date=gdate,Value=gval)\nX.Value[gndxmissing] .= missing\n\ndnnr = DateValLinearImputer()\nfit!(dnnr,X)\ntransform!(dnnr,X)\n\nImplements: fit!, transform!`\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.DateValMultiNNer","page":"Types and Functions","title":"TSML.ValDateFilters.DateValMultiNNer","text":"DateValMultiNNer(\n   Dict(\n      :type => :knn # :linear\n      :missdirection => :symmetric, #:reverse, # or :forward or :symmetric\n      :dateinterval => Dates.Hour(1),\n      :nnsize => 1,\n      :strict => true,\n      :aggregator => :median\n  )\n)\n\nFills missings with their nearest-neighbors. It assumes that first column is a Date class and the other columns are Union{Missings,Real}. It uses DateValNNer and DateValizer+Impute to process each numeric column concatendate with the Date column.\n\n:type => type of imputation which can be a linear interpolation or nearest neighbor\n:missdirection => direction to fill missing data (:symmetric, :reverse, :forward) \n:dateinterval => time period to use for grouping,\n:nnsize => neighborhood size,\n:strict => boolean value to indicate whether to be strict about replacement or not,\n`:aggregator => function to aggregate based on date interval\n\nExample:\n\nRandom.seed!(123)\ngdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\ngval1 = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngval2 = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngval3 = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngmissing = 50000\ngndxmissing1 = Random.shuffle(1:length(gdate))[1:gmissing]\ngndxmissing2 = Random.shuffle(1:length(gdate))[1:gmissing]\ngndxmissing3 = Random.shuffle(1:length(gdate))[1:gmissing]\nX = DataFrame(Date=gdate,Temperature=gval1,Humidity=gval2,Ozone=gval3)\nX.Temperature[gndxmissing1] .= missing\nX.Humidity[gndxmissing2] .= missing\nX.Ozone[gndxmissing3] .= missing\n\ndnnr = DateValMultiNNer(Dict(\n      :type=>:linear,\n      :dateinterval=>Dates.Hour(1),\n      :nnsize=>10,\n      :missdirection => :symmetric,\n      :strict=>true,\n      :aggregator => :mean))\nfit!(dnnr,X)\ntransform!(dnnr,X)\n\nImplements: fit!, transform!`\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.DateValNNer","page":"Types and Functions","title":"TSML.ValDateFilters.DateValNNer","text":"DateValNNer(\n   Dict(\n      :missdirection => :symmetric, #:reverse, # or :forward or :symmetric\n      :dateinterval => Dates.Hour(1),\n      :nnsize => 1,\n      :strict => true,\n      :aggregator => :median\n  )\n)\n\nFills missings with their nearest-neighbors.\n\n:missdirection => direction to fill missing data (:symmetric, :reverse, :forward) \n:dateinterval => time period to use for grouping,\n:nnsize => neighborhood size,\n:strict => boolean value to indicate whether to be strict about replacement or not,\n`:aggregator => function to aggregate based on date interval\n\nExample:\n\nRandom.seed!(123)\ngdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\ngval = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngmissing = 50000\ngndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\nX = DataFrame(Date=gdate,Value=gval)\nX.Value[gndxmissing] .= missing\n\ndnnr = DateValNNer(Dict(\n      :dateinterval=>Dates.Hour(1),\n      :nnsize=>10,\n      :missdirection => :symmetric,\n      :strict=>true,\n      :aggregator => :mean))\nfit!(dnnr,X)\ntransform!(dnnr,X)\n\nImplements: fit!, transform!`\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.DateValgator","page":"Types and Functions","title":"TSML.ValDateFilters.DateValgator","text":"DateValgator(args=Dict())\n   Dict(\n    :dateinterval => Dates.Hour(1),\n    :aggregator => :median\n  )\n)\n\nAggregates values based on date period specified.\n\nExample:\n\n# generate random values with missing data\nRandom.seed!(123)\ngdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\ngval = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngmissing = 50000\ngndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\nX = DataFrame(Date=gdate,Value=gval)\nX.Value[gndxmissing] .= missing\n\ndtvlmean = DateValgator(Dict(\n      :dateinterval=>Dates.Hour(1),\n      :aggregator => :mean))\nfit!(dtvlmean,X)\nres = transform!(dtvlmean,X)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.DateValizer","page":"Types and Functions","title":"TSML.ValDateFilters.DateValizer","text":"DateValizer(\n   Dict(\n    :medians => DataFrame(),\n    :dateinterval => Dates.Hour(1)\n  )\n)\n\nNormalizes and cleans time series by replacing missings with global medians  computed based on time period groupings.\n\nExample:\n\n# generate random values with missing data\nRandom.seed!(123)\ngdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\ngval = Array{Union{Missing,Float64}}(rand(length(gdate)))\ngmissing = 50000\ngndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\nX = DataFrame(Date=gdate,Value=gval)\nX.Value[gndxmissing] .= missing\n\ndvzr = DateValizer(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(dvzr,X)\ntransform!(dvzr,X)\n\nImplements: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.Dateifier","page":"Types and Functions","title":"TSML.ValDateFilters.Dateifier","text":"Dateifier(args=Dict())\n   Dict(\n    :ahead => 1,\n    :size => 7,\n    :stride => 1\n   )\n)\n\nConverts a 1-D date series into sliding window matrix for ML training\n\nExample: \n\ndtr = Dateifier(Dict())\nlower = DateTime(2017,1,1)\nupper = DateTime(2018,1,31)\ndat=lower:Dates.Day(1):upper |> collect\nvals = rand(length(dat))\nx=DataFrame(Date=dat,Value=vals)\nfit!(dtr,x)\nres = transform!(dtr,x)\n\nImplements: 'fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#TSML.ValDateFilters.Matrifier","page":"Types and Functions","title":"TSML.ValDateFilters.Matrifier","text":"Matrifier(Dict(\n   Dict(\n    :ahead  => 1,\n    :size   => 7,\n    :stride => 1,\n  )\n)\n\nConverts a 1-D timeseries into sliding window matrix for ML training:\n\n:ahead  => steps ahead to predict\n:size   => size of sliding window\n:stride => amount of overlap in sliding window\n\nExample:\n\nmtr = Matrifier(Dict(:ahead=>24,:size=>24,:stride=>5))\nlower = DateTime(2017,1,1)\nupper = DateTime(2017,1,5)\ndat=lower:Dates.Hour(1):upper |> collect\nvals = 1:length(dat)\nx = DataFrame(Date=dat,Value=vals)\nfit!(mtr,x)\nres = transform!(mtr,x)\n\nImplements: fit!, transform\n\n\n\n\n\n","category":"type"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-8","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(mtr::Matrifier,xx::T,y::Vector=Vector()) where {T<:Union{Matrix,Vector,DataFrame}}\n\nChecks and validate inputs are in correct structure\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-9","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dvzr::DateValizer,xx::T,y::Vector=[]) where {T<:DataFrame}\n\nValidates input and computes global medians grouped by time period.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-10","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(csvwtr::CSVDateValWriter,x::T=[],y::Vector=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n\nMakes sure filename and dateformat are not empty strings.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-11","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dtr::Dateifier,xx::T,y::Vector=[]) where {T<:Union{Matrix,Vector,DataFrame}}\n\nComputes range of dates to be used during transform.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-12","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dnnr::DateValMultiNNer,xx::T,y::Vector=[]) where {T<:DataFrame}\n\nValidates and checks arguments for errors.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-13","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(csvrdr::CSVDateValReader,x::T=[],y::Vector=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n\nMakes sure filename and dateformat are not empty strings.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-14","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dvmr::DateValgator,xx::T,y::Vector=[]) where {T<:Union{Matrix,DataFrame}}\n\nChecks and validates arguments.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-15","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dnnr::DateValLinearImputer,xx::T,y::Vector=[]) where {T<:DataFrame}\n\nValidates and checks arguments for errors.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.fit!-16","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.fit!","text":"fit!(dnnr::DateValNNer,xx::T,y::Vector=[]) where {T<:DataFrame}\n\nValidates and checks arguments for errors.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-3","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(csvrdr::CSVDateValReader,x::T=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n\nUses CSV package to read the csv file and converts it to dataframe.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{CSVDateValWriter,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(csvwtr::CSVDateValWriter,x::T) where {T<:Union{DataFrame,Vector,Matrix}}\n\nUses CSV package to write the dataframe into a csv file.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{DateValLinearImputer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dnnr::DateValLinearImputer,xx::T) where {T<:DataFrame}\n\nReplaces missings by linear interpolation.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{DateValMultiNNer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dnnr::DateValMultiNNer,xx::T) where {T<:DataFrame}\n\nReplaces missings by nearest neighbor or linear interpolation by looping over the dataset  for each column until all missing values are gone.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{DateValNNer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dnnr::DateValNNer,xx::T) where {T<:DataFrame}\n\nReplaces missings by nearest neighbor looping over the dataset until all missing values are gone.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{DateValgator,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dvmr::DateValgator,xx::T) where {T<:DataFrame}\n\nAggregates values grouped by date-time period using aggregate  function such as mean, median, maximum, minimum. Default is mean.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{DateValizer,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dvzr::DateValizer,xx::T) where {T<:DataFrame}\n\nReplaces missing with the corresponding global medians with respect to time period.\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Dateifier,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(dtr::Dateifier,xx::T) where {T<:Union{Matrix,Vector,DataFrame}}\n\nTransforms to day of the month, day of the week, etc\n\n\n\n\n\n","category":"method"},{"location":"lib/functions/#AMLPipelineBase.AbsTypes.transform!-Tuple{Matrifier,DataFrame}","page":"Types and Functions","title":"AMLPipelineBase.AbsTypes.transform!","text":"transform!(mtr::Matrifier,xx::T) where {T<:Union{Matrix,Vector,DataFrame}}\n\nApplies the parameters of sliding windows to create the corresponding matrix\n\n\n\n\n\n","category":"method"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/dateproc/#Date-Preprocessing","page":"Date PreProcessing","title":"Date Preprocessing","text":"","category":"section"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"Extracting the Date features in a Date,Value table follows similar workflow with the value preprocessing  of the previous section. The main difference  is we are only interested on the date corresponding to the last column of the values generated by the Matrifier. This last column contains the values before  the prediction happens and the dates corresponding to these values carry significant information based on recency compared to the other dates.","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"Let us start by creating a Date,Value dataframe similar to the previous section.","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"using TSML\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2018,1,31)\ndat=lower:Dates.Day(1):upper |> collect\nvals = rand(length(dat))\nx = DataFrame(Date=dat,Value=vals)\nnothing #hide","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"first(x,5)","category":"page"},{"location":"man/dateproc/#Dateifier","page":"Date PreProcessing","title":"Dateifier","text":"","category":"section"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"Let us create an instance of Dateifier passing the size of row, stride, and steps ahead to predict:","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"mtr = Dateifier(Dict(:ahead=>24,:size=>24,:stride=>5))\nres = fit_transform!(mtr,x)\nnothing #hide","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"first(res,5)","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"The model transform! output extracts automatically several date features such as year, month, day, hour, week, day of the week,  day of quarter, quarter of year.","category":"page"},{"location":"man/dateproc/#ML-Features:-Matrifier-and-Datefier","page":"Date PreProcessing","title":"ML Features: Matrifier and Datefier","text":"","category":"section"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"You can then combine the outputs in both the Matrifier and Datefier  as input features to a machine learning model. Below is an example of the workflow where the code extracts the Date and Value features combining them to form a matrix of features as input to a machine learning model.","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"commonargs = Dict(:ahead=>3,:size=>5,:stride=>2)\ndtr = Dateifier(commonargs)\nmtr = Matrifier(commonargs)\n\nlower = DateTime(2017,1,1)\nupper = DateTime(2018,1,31)\ndat=lower:Dates.Day(1):upper |> collect\nvals = rand(length(dat))\nX = DataFrame(Date=dat,Value=vals)\n\nvaluematrix = fit_transform!(mtr,X)\ndatematrix = fit_transform!(dtr,X)\nmlfeatures = hcat(datematrix,valuematrix)\nnothing #hide","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"first(mlfeatures,5)","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"Another way is to use the symbolic pipeline to describe the transformation and concatenation in just one line of expression.","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"ppl = @pipeline dtr + mtr\nfeatures = fit_transform!(ppl,X)\nnothing #hide","category":"page"},{"location":"man/dateproc/","page":"Date PreProcessing","title":"Date PreProcessing","text":"first(features,5)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Monotonic-Detection-and-Plotting","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"One important preprocessing step for time series data processing is the detection  of monotonic data and transform it to non-monotonic type by using the finite difference operator.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Artificial-Data-Example","page":"Monotonic Detection and Plotting","title":"Artificial Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's create an artificial monotonic data and apply our monotonic transformer to normalize it. We can use the Plotter filter to visualize the generated data.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nRandom.seed!(123)\npltr = Plotter(Dict(:interactive => false,:pdfoutput => true))\nmdates = DateTime(2017,12,1,1):Dates.Hour(1):DateTime(2017,12,31,10) |> collect\nmvals = rand(length(mdates)) |> cumsum\ndf =  DataFrame(Date=mdates ,Value = mvals)\nfit_transform!(pltr,df)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Now that we have a monotonic data, let's use the Monotonicer to normalize and plot the result:","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nmono = Monotonicer(Dict())\n\npipeline = @pipeline mono |> pltr\n\nres=fit_transform!(pipeline,df)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Real-Data-Example","page":"Monotonic Detection and Plotting","title":"Real Data Example","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We will now apply the entire pipeline  starting from reading csv data, aggregate, impute, normalize if it's monotonic, and plot. We will consider three  different data types: a regular time series data, a   monotonic data, and a daily monotonic data. The difference between   monotonic and daily monotonic is that the values in daily monotonic resets to  zero or some baseline and cumulatively increases in a day until the  next day where it resets to zero or some baseline value. Monotonicer automatically detects these three different types and apply the corresponding normalization accordingly.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML\n\nregularfile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/regular.csv\")\nmonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/monotonic.csv\")\ndailymonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/dailymonotonic.csv\")\n\nregularfilecsv = CSVDateValReader(Dict(:filename=>regularfile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nmonofilecsv = CSVDateValReader(Dict(:filename=>monofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\ndailymonofilecsv = CSVDateValReader(Dict(:filename=>dailymonofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\n\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\npltr = Plotter(Dict(:interactive => false))\nnothing #hide","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Regular-TS-Processing","page":"Monotonic Detection and Plotting","title":"Regular TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's test by feeding the regular time series type to the pipeline. We expect that for this type, Monotonicer will not perform further processing:","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline regularfilecsv |> valgator |> valnner |> mono |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline regularfilecsv |> valgator |> valnner |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that the plots are the same with or without the Monotonicer instance.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Monotonic-TS-Processing","page":"Monotonic Detection and Plotting","title":"Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's now feed the same pipeline with a monotonic csv data.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline monofilecsv |> valgator |> valnner |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline monofilecsv |> valgator |> valnner |> mono |> pltr\n\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Notice that without the Monotonicer instance, the data is monotonic. Applying the Monotonicer instance in the pipeline converts the data into a regular time series but with outliers.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"We can use the Outliernicer filter to remove outliers. Let's apply this filter after the Monotonicer and plot the result.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"using TSML: Outliernicer\noutliernicer = Outliernicer(Dict(:dateinterval=>Dates.Hour(1)));\n\npipeline = @pipeline monofilecsv |> valgator |> valnner |> mono |>  outliernicer |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/#Daily-Monotonic-TS-Processing","page":"Monotonic Detection and Plotting","title":"Daily Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Lastly, let's feed the daily monotonic data using similar pipeline and examine its plot.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline without Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"This plot is characterized by monotonically increasing trend but resets to certain baseline value  at the end of the day and repeat similar trend daily. The challenge for the monotonic normalizer is to differentiate between daily monotonic from the typical monotonic function to apply the correct normalization.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> mono |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"While the Monotonicer filter is able to transform the data into a regular time series, there are significant outliers due to noise and the nature of this kind of data or sensor.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Let's remove the outliers by applying the Outliernicer filter and examine the result.","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"Pipeline with Monotonicer and Outliernicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"pipeline = @pipeline dailymonofilecsv |> valgator |> valnner |> mono |> outliernicer |> pltr\nfit_transform!(pipeline)","category":"page"},{"location":"tutorial/monotonic_plotting_pdf/","page":"Monotonic Detection and Plotting","title":"Monotonic Detection and Plotting","text":"The Outliernicer filter effectively removed the outliers as shown in the plot.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/tsclassifier/#TS-Data-Discovery","page":"TS Data Discovery","title":"TS Data Discovery","text":"","category":"section"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"We have enough building blocks to perform data discovery given a bunch  of time series data generated by sensors. Processing hundreds or thousands of time series data is becoming a common occurrence and typical challenge nowadays with the rapid adoption of IoT technology in buildings, manufacturing industries, etc.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this section, we will use those transformers discussed in the previous sections to normalize and extract the statistical features of TS. These extracted stat features will be used as input to a Machine learning model. We will train this model to learn the signatures of different TS types so that we can use it to classify unknown or unlabeled sensor data.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this tutorial, we will use TSClassifier which works in the following context:  Given a bunch of time-series with specific types. Get the statistical features of each, use these as inputs to a classifier with output as the TS type, train, and test. Another option is to use these stat features for clustering and check cluster quality. If accuracy is poor, add more stat features and repeat same process as outlined for training and testing. Assume that each time series during training is named based on their type which will be used as the target output. For example, temperature time series will be named as temperature?.csv where ? is any positive integer. Using this setup, the TSClassifier loops over each file in the training directory, get the stats and record these accumulated stat features into a dataframe and train the model to learn the input->output mapping during fit! operation. Apply the learned models in the transform! operation loading files in the testing directory.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"The entire process of training to learn the appropriate parameters and classification to identify unlabeled data exploits the idea of the pipeline workflow discussed in the previous sections.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's illustrate the process by loading some sample data:","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"using TSML\n\nRandom.seed!(12345)\ntrdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/training\")\ntstdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/testing\")\nmodeldirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/model\")\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Here's the list of files for training:","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(trdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"and here are the files in testing directory:","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(tstdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"The files in testing directory doesn't need to be labeled but we use the labeling as a way to validate the effectiveness of the classifier. The labels will be used as the groundtruth during prediction/classification.","category":"page"},{"location":"tutorial/tsclassifier/#TSClassifier","page":"TS Data Discovery","title":"TSClassifier","text":"","category":"section"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let us now setup an instance of the TSClassifier and pass the arguments containing the directory locations of files for training, testing, and modeling.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"using TSML\n\ntscl = TSClassifier(Dict(:trdirectory=>trdirname,\n          :tstdirectory=>tstdirname,\n          :modeldirectory=>modeldirname,\n          :feature_range => 6:20,\n          :num_trees=>20)\n       )\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Time to train our TSClassifier to learn the mapping between extracted stats features with the  TS type.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"fit!(tscl);\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"We can examine the extracted features saved by the model that is used for its training.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"mdirname = tscl.model[:modeldirectory]\nmodelfname=tscl.model[:juliarfmodelname]\n\ntrstatfname = joinpath(mdirname,modelfname*\".csv\")\nres = CSV.read(trstatfname) |> DataFrame\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"first(res,5)","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's check the accuracy of prediction with the test data using the transform! function.","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"dfresults = transform!(tscl);\nfirst(dfresults,5)","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"The table above shows the prediction corresponding to each filename which is the groundtruth. We can compute the accuracy by extracting from the filename the TS type and compare it with the corresponding prediction. Below computes the prediction accuracy:","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"prediction = dfresults.predtype\nfnames = dfresults.fname\nmyregex = r\"(?<dtype>[A-Z _ - a-z]+)(?<number>\\d*).(?<ext>\\w+)\"\ngroundtruth=map(fnames) do fname\n  mymatch=match(myregex,fname)\n  mymatch[:dtype]\nend\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"sum(groundtruth .== prediction) / length(groundtruth) * 100","category":"page"},{"location":"tutorial/tsclassifier/","page":"TS Data Discovery","title":"TS Data Discovery","text":"Of course we need more data to split between training and testing to improve accuracy and get a more stable measurement of performance.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"#TSML-(Time-Series-Machine-Learning)","page":"HOME","title":"TSML (Time-Series Machine Learning)","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"TSML (Time Series Machine Learning) is a package  for Time Series data processing, classification, and prediction. It combines ML libraries from Python's  ScikitLearn, R's Caret, and Julia ML using a common API  and allows seamless ensembling and integration of  heterogenous ML libraries to create complex models  for robust time-series pre-processing and prediction/classification.","category":"page"},{"location":"#Motivations","page":"HOME","title":"Motivations","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Over the past years, the industrial sector has seen  many innovations brought about by automation.  Inherent in this automation is the installation of  sensor networks for status monitoring and data collection.  One of the major challenges in these data-rich  environments is how to extract and exploit  information from these large volume of data to  detect anomalies, discover patterns to reduce  downtimes and manufacturing errors, reduce energy usage, etc.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"To address these issues, we developed TSML package.  It leverages AI and ML libraries from ScikitLearn, Caret,  and Julia as building blocks in processing huge amount of  industrial time series data. It has the following characteristics  described below.","category":"page"},{"location":"#Package-Features","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"TS data type clustering/classification for automatic data discovery\nTS aggregation based on date/time interval\nTS imputation based on Nearest Neighbors\nTS statistical metrics for data quality assessment\nTS ML wrapper more than 100+ libraries from caret, scikitlearn, and julia\nTS date/value matrix conversion of 1-D TS using sliding windows for ML input\nCommon API wrappers for ML libs from JuliaML, PyCall, and RCall\nPipeline API allows high-level description of the processing workflow\nSpecific cleaning/normalization workflow based on data type\nAutomatic selection of optimised ML model\nAutomatic segmentation of time-series data into matrix form for ML training and  prediction\nEasily extensible architecture by using just two main interfaces: fit and transform\nMeta-ensembles for robust prediction\nSupport for distributed computation for scalability and speed","category":"page"},{"location":"#Installation","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"TSML is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add TSML","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add TSML\"","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"TSML\")","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> pkg\"add TSML\"","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Once TSML is installed, you can load the TSML package by:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> using TSML","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"julia> import TSML","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Generally, you will need the different transformers and utils in TSML for time-series processing. To use them, TSML relies on the Reexport.jl package to exposes all the necessary filters and transformers into the Julia Main module including exported functions in DataFrames, CSV, Dates, and Random. By just a single line below, all these related modules become available in Julia Main module:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"using TSML ","category":"page"},{"location":"#Tutorial-Outline","page":"HOME","title":"Tutorial Outline","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"tutorial/aggregators.md\",\n  \"tutorial/pipeline.md\",\n  \"tutorial/statistics.md\",\n  \"tutorial/monotonic_plotting.md\",\n  \"tutorial/tsclassifier.md\"\n]\nDepth = 3","category":"page"},{"location":"#Manual-Outline","page":"HOME","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"man/valueproc.md\",\n  \"man/dateproc.md\",\n  \"man/aggregation.md\",\n  \"man/imputation.md\",\n]\nDepth = 3","category":"page"},{"location":"#ML-Library","page":"HOME","title":"ML Library","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Pages = [\n  \"lib/functions.md\"\n]","category":"page"}]
}
